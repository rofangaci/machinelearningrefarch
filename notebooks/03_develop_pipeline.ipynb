{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will develop our Azure Machine Learning Pipeline. The Azure Machine learning pipeline will string together the steps of preprocessing the video, applying style transfer, and postprocessing the video into a single execution graph. \n",
    "\n",
    "To setup the pipeline, we'll need to make sure we have the necessary compute and storage available. To do so, we'll need to create our compute platform using AmlCompute and register the storage account that we created in the previous notebook.\n",
    "\n",
    "The last step of this notebook is to publish the pipeline. Once it's published as a public endpoint, we'll test it to make sure that it runs as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import package and load .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import set_key, get_key, find_dotenv, load_dotenv\n",
    "from pathlib import Path\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, MpiStep\n",
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE #, DEFAULT_GPU_IMAGE\n",
    "from IPython.core.display import display, HTML\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_path = find_dotenv(raise_error_if_not_found=True)\n",
    "load_dotenv(env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the workspace in AML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get our workspace from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "stripout"
    ]
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "# Also create a Project and attach to Workspace\n",
    "project_folder = \"scripts\"\n",
    "run_history_name = project_folder\n",
    "\n",
    "if not os.path.isdir(project_folder):\n",
    "    os.mkdir(project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our compute using `AmlCompute`. We'll need one node for the video pre/post processing. And the remaining nodes for performing the style transfer. Since we'll be using the MPI Step, all nodes must be active before the MPI step will execute. Thus, we should set max nodes to equal min nodes, as there is no point autoscaling the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of nodes we want for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_transfer_node_count = 4\n",
    "ffmpeg_node_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the subscription in use has enough cores. We need to check for two vm types since we'll be using NCSv2 for style transfer and DSv2 for ffmpeg processes. If you do not have quota for the NCSv2 family, you can use another GPU family instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_dict = {\n",
    "    \"NC\": {\n",
    "        \"size\": \"STANDARD_NC6\",\n",
    "        \"cores\": 6\n",
    "    },\n",
    "    \"NCSv3\": {\n",
    "        \"size\": \"STANDARD_NC6s_v3\",\n",
    "        \"cores\": 6\n",
    "    },\n",
    "    \"DSv2\": {\n",
    "        \"size\": \"STANDARD_DS3_V2\",\n",
    "        \"cores\": 4\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our non-gpu DSv2 cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ffmpeg-cluster\n",
      "Creating\n",
      "Succeeded..............\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "# CPU compute\n",
    "cpu_cluster_name = \"ffmpeg-cluster\"\n",
    "try:\n",
    "    cpu_cluster = AmlCompute(ws, cpu_cluster_name)\n",
    "    print(\"Found existing cluster.\")\n",
    "except:\n",
    "    print(\"Creating {}\".format(cpu_cluster_name))\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=vm_dict[\"DSv2\"][\"size\"], \n",
    "        min_nodes=ffmpeg_node_count, \n",
    "        max_nodes=ffmpeg_node_count\n",
    "    )\n",
    "\n",
    "    # create the cluster\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, provisioning_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our NCSv2 cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating style-cluster\n",
      "Creating\n",
      "Succeeded.......................\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "# GPU compute\n",
    "gpu_cluster_name = \"style-cluster\"\n",
    "try:\n",
    "    gpu_cluster = AmlCompute(ws, gpu_cluster_name)\n",
    "    print(\"Found existing cluster.\")\n",
    "except:\n",
    "    print(\"Creating {}\".format(gpu_cluster_name))\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=vm_dict[\"NC\"][\"size\"], \n",
    "        min_nodes=style_transfer_node_count, \n",
    "        max_nodes=style_transfer_node_count\n",
    "    )\n",
    "\n",
    "    # create the cluster\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, provisioning_config)\n",
    "    gpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the default data store provided by the Azure Machine Learning workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_datastore = ws.get_default_datastore()\n",
    "set_key(env_path, \"AML_DATASTORE_NAME\", \"workspaceblobstore\")\n",
    "set_key(env_path, \"STORAGE_ACCOUNT_NAME\", my_datastore.account_name)\n",
    "set_key(env_path, \"STORAGE_ACCOUNT_KEY\", my_datastore.account_key)\n",
    "set_key(env_path, \"STORAGE_CONTAINER_NAME\", my_datastore.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the `models` folder (from out local directory) and the `orangutan.mp4` video to the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget -O orangutan.mp4 https://happypathspublic.blob.core.windows.net/assets/batch_scoring_for_dl/input_video.mp4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_datastore"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload files in models folder to a directory called models\n",
    "my_datastore.upload_files(\n",
    "    [\"./models/model.pth\"],\n",
    "    target_path=\"models\", \n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Upload orangutan.mp4 video\n",
    "my_datastore.upload_files(\n",
    "    [\"./orangutan.mp4\"],\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `models` dir we uploaded as data references to be used by the pipeline steps later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = DataReference(\n",
    "    data_reference_name=\"model_dir\", \n",
    "    datastore=my_datastore, \n",
    "    path_on_datastore=\"models\", \n",
    "    mode=\"download\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the output video to be saved in the same datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video = PipelineData(name=\"output_video\", datastore=my_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a reference to the datastore that was generated when the AML workspace was created. We'll use this datastore to hold temporary pipeline data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_datastore = ws.get_default_datastore()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save all temporary data files (PipelineData) to the default datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_audio = PipelineData(name=\"ffmpeg_audio\", datastore=default_datastore)\n",
    "ffmpeg_images = PipelineData(name=\"ffmpeg_images\", datastore=default_datastore)\n",
    "processed_images = PipelineData(name=\"processed_images\", datastore=default_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup cluster environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config for ffmpeg cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_cd = CondaDependencies()\n",
    "ffmpeg_cd.add_channel(\"conda-forge\")\n",
    "ffmpeg_cd.add_conda_package(\"ffmpeg\")\n",
    "\n",
    "ffmpeg_run_config = RunConfiguration(conda_dependencies=ffmpeg_cd)\n",
    "ffmpeg_run_config.environment.docker.enabled = True\n",
    "ffmpeg_run_config.environment.docker.gpu_support = False\n",
    "ffmpeg_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "ffmpeg_run_config.environment.spark.precache_packages = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config for style transfer cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_transfer_cd = CondaDependencies()\n",
    "style_transfer_cd.add_channel(\"pytorch\")\n",
    "style_transfer_cd.add_conda_package(\"pytorch\")\n",
    "\n",
    "style_transfer_run_config = RunConfiguration(conda_dependencies=style_transfer_cd)\n",
    "style_transfer_run_config.environment.docker.enabled = True\n",
    "style_transfer_run_config.environment.docker.gpu_support = True\n",
    "style_transfer_run_config.environment.docker.base_image = \"pytorch/pytorch\"\n",
    "style_transfer_run_config.environment.spark.precache_packages = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pipeline steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting up the pipelines, we'll need to create a `video_path_param` that can be modified when the pipeline is published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path_default = DataPath(datastore=my_datastore, path_on_datastore=\"orangutan.mp4\")\n",
    "video_path_param = (PipelineParameter(name=\"video_path\", default_value=video_path_default), DataPathComputeBinding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the 3-step pipeline using PythonScriptSteps and the MpiStep. In the MPI step, you'll notice that we use the `style_transfer_mpi.py` script instead of the `style_transfer.py` script. This is because the MPI expects that the script is modified to use MPI code.\n",
    "\n",
    "Both scripts do the exact same thing, except that the `style_transfer_mpi.py` script is set up to use MPI to run process the frames in a distributed way. \n",
    "\n",
    "Feel free to inspect the differences under the `scripts` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_video_step = PythonScriptStep(\n",
    "    name=\"preprocess video\",\n",
    "    script_name=\"preprocess_video.py\",\n",
    "    arguments=[\"--input-video\", video_path_param,\n",
    "               \"--output-audio\", ffmpeg_audio,\n",
    "               \"--output-images\", ffmpeg_images,\n",
    "              ],\n",
    "    compute_target=cpu_cluster,\n",
    "    inputs=[video_path_param],\n",
    "    outputs=[ffmpeg_images, ffmpeg_audio],\n",
    "    runconfig=ffmpeg_run_config,\n",
    "    source_directory=project_folder,\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "distributed_style_transfer_step = MpiStep(\n",
    "    name=\"mpi style transfer\",\n",
    "    script_name=\"style_transfer_mpi.py\",\n",
    "    arguments=[\"--content-dir\", ffmpeg_images,\n",
    "               \"--output-dir\", processed_images,\n",
    "               \"--model-dir\", model_dir,\n",
    "               \"--cuda\", 1\n",
    "              ],\n",
    "    compute_target=gpu_cluster,\n",
    "    node_count=4, \n",
    "    process_count_per_node=1,\n",
    "    inputs=[model_dir, ffmpeg_images],\n",
    "    outputs=[processed_images],\n",
    "    pip_packages=[\"image\", \"mpi4py\", \"torch\", \"torchvision\"],\n",
    "    runconfig=style_transfer_run_config,\n",
    "    use_gpu=True,\n",
    "    source_directory=project_folder,\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "postprocess_video_step = PythonScriptStep(\n",
    "    name=\"postprocess video\",\n",
    "    script_name=\"postprocess_video.py\",\n",
    "    arguments=[\"--images-dir\", processed_images, \n",
    "               \"--input-audio\", ffmpeg_audio, \n",
    "               \"--output-dir\", output_video],\n",
    "    compute_target=cpu_cluster,\n",
    "    inputs=[processed_images, ffmpeg_audio],\n",
    "    outputs=[output_video],\n",
    "    runconfig=ffmpeg_run_config,\n",
    "    source_directory=project_folder,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline, passing in the video path variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step postprocess video [7cc17697][f9be7548-48d6-4df0-99f6-99a0c0377c20], (This step will run and generate new outputs)\n",
      "Created step mpi style transfer [e7ba080e][3017c382-f3cf-4055-8f20-409ac9305ffc], (This step will run and generate new outputs)\n",
      "Created step preprocess video [2bb7ac9c][31623158-d45d-4c8a-8fcc-c12d4a73f4b6], (This step will run and generate new outputs)\n",
      "Created data reference model_dir for StepId [416f19e6][88eeea78-4063-4fc8-9930-5480186dd516], (Consumers of this data will generate new runs.)\n",
      "Created data reference datastore_975052cf_ad24f844 for StepId [17a8ad15][4c81af2a-45ec-47b5-8678-e08556c3738a], (Consumers of this data will generate new runs.)\n",
      "Submitted pipeline run: 304146d1-ea9c-445c-978a-b8c55800759a\n"
     ]
    }
   ],
   "source": [
    "steps = [postprocess_video_step]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'style_transfer_mpi').submit(\n",
    "    pipeline, \n",
    "    pipeline_params={'video_path': DataPath(datastore=my_datastore, path_on_datastore=\"orangutan.mp4\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>style_transfer_mpi</td><td>304146d1-ea9c-445c-978a-b8c55800759a</td><td>azureml.PipelineRun</td><td>NotStarted</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/jiataamltest02/providers/Microsoft.MachineLearningServices/workspaces/jiataamltest02/experiments/style_transfer_mpi/runs/304146d1-ea9c-445c-978a-b8c55800759a\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: style_transfer_mpi,\n",
       "Id: 304146d1-ea9c-445c-978a-b8c55800759a,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: NotStarted)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the pipeline completes before proceeding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status:Running\n",
      "...............................................................................................................................................................................................................................................................\n",
      "status:Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the output video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the step id of the postprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_id = pipeline_run.find_step_run(\"postprocess video\")[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the output files from the postprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_datastore.download(\n",
    "    target_path=\"aml_test_orangutan\", \n",
    "    prefix=step_id, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the generated output video that we just downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"320\" height=\"240\" controls>\n",
       "        <source src=\"aml_test_orangutan/304146d1ea9c445c978ab8c55800759a_7cc17697_1-30-2019_07-39-13_AM/output_video/video_processed.mp4\" type=\"video/mp4\">\n",
       "    </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"\"\"\n",
    "    <video width=\"320\" height=\"240\" controls>\n",
    "        <source src=\"aml_test_orangutan/{}/output_video/video_processed.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\".format(step_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to publish the pipeline so that the pipeline can be triggered on an http endpoint. We'll use Logic Apps in the next notebook to consume this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(\n",
    "    name=\"style transfer\", \n",
    "    description=\"some description\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "stripout"
    ]
   },
   "outputs": [],
   "source": [
    "published_pipeline_id = published_pipeline.id\n",
    "set_key(env_path, \"AML_PUBLISHED_PIPELINE_ID\", published_pipeline_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the published pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "stripout"
    ]
   },
   "outputs": [],
   "source": [
    "cli_auth = AzureCliAuthentication()\n",
    "aad_token = cli_auth.get_authentication_header()\n",
    "\n",
    "response = requests.post(\n",
    "    published_pipeline.endpoint, \n",
    "    headers=aad_token, \n",
    "    json={\n",
    "        \"ExperimentName\": \"My_Pipeline\",\n",
    "        \"DataPathAssignments\": {\n",
    "            \"video_path\": {\"DataStoreName\": \"workspaceblobstore\",\n",
    "                           \"RelativePath\": \"orangutan.mp4\"}\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "run_id = response.json()[\"Id\"]\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to move on to the [next notebook](04_deploy_logic_apps.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:batchscoringdl_aml]",
   "language": "python",
   "name": "conda-env-batchscoringdl_aml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
